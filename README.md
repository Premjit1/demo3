# WebCam-Face-Emotion-Detection-Streamlit
Real time face detection streamlit based bew application for server deployment.
We will solve the before-mentioned challenge by applying deep learning 
algorithms to live video data. The solution to this problem is
by recognizing facial emotions
DESCRIPTION OF DATA SET
• The FER-2013 dataset consists of 28,000 labeled images in the training set, 3,500 labeled images 
in the
development set, and 3,500 images in the test set. Each image in FER-2013 is labeled as one of 
seven emotions:
happy, sad, angry, afraid, surprise, disgust, and neutral, with happy being the most prevalent emotion, 
providing a
baseline for random guessing of 24.4%.
• The images in FER-2013 consist of both posed and unposed headshots, which are in grayscale and 
48x48 pixels.
• The FER-2013 dataset was created by gathering the results of a Google image search of each 
emotion and
synonyms of the emotions
USE OF THIS PROJECT

• In a physical classroom during a lecturing teacher can see the faces and assess the emotion of the class and tune their lecture 
accordingly, whether he is going fast or slow. He can identify students who need special attention. Digital classrooms are conducted 
via video telephony software program (exZoom) where it’s not possible for medium scale class (25-50) to see all students and access 
the mood. Because of this drawback, students are not focusing on content due to lack of surveillance.
• While digital platforms have limitations in terms of physical surveillance but it comes with the power of data and machines which can 
work for you. It provides data in the form of video, audio, and texts which can be analysed using deep learning algorithms. Deep 
learning backed system not only solves the surveillance issue, but it also removes the human bias from the system, and all information 
is no longer in the teacher’s brain rather translated in numbers that can be analysed and tracked.
What is Facial Emotion Recognition?
• Facial emotion recognition is the process of detecting human emotions from facial expressions. The 
human brain recognizes emotions automatically, and software has now been developed that can 
recognize emotions as well. This technology is becoming more accurate all the time, and will eventually 
be able to read emotions as well as our brains do. 
• AI can detect emotions by learning what each facial expression means and applying that knowledge to 
the new information presented to it. Emotional artificial intelligence, or emotion AI, is a technology that is 
capable of reading, imitating, interpreting, and responding to human facial expressions and emotions. 
●
Other Applications of Facial Emotion Recognition
• Market Research: Companies have traditionally done market research by conducting surveys to find 
out about what consumers want and need. This method however, assumes that the preferences stated 
are correct and reflect future actions. But this is not always the case. Another popular approach in 
market research is to employ behavioral methods where user’s reactions are observed, while 
interacting with a brand or a product. Although effective, such techniques can quickly become very 
labor intensive as the sample size increases. In such circumstances, facial expression recognition 
technology can save the day by allowing companies to conduct market research and measure 
moment-by-moment facial expressions of emotions automatically, making it easy aggregate the results
• VideoGame Testing: Facial expression recognition can also be used in the video game testing phase. 
In this phase, usually a focus group of users is asked to play a game for a given amount of time and 
their behavior and emotions are monitored. By using facial expression recognition, game developers 
can gain insights and draw conclusions about the emotions experienced during game play and 
incorporate that feedback in the making of the final product. Facial expression analysis is a practical 
means of going beyond the typical survey approach. It is a way of appreciating what the user is 
experiencing, all while getting feedback. When feedback is taken in this format, it becomes genuinely 
non-intrusive when it comes to user experience
About our DataSet
• The FER-2013 dataset consists of 28,000 labeled images in the training set, 3,500 labeled images in the 
development set, and 3,500 images in the test set. Each image in FER-2013 is labeled as one of seven emotions
